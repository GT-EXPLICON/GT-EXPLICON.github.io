<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>GT EXPLICON</title>
<meta http-equiv="Content-Language" content="English" />
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
<link rel="stylesheet" type="text/css" href="style.css" media="screen" />
</head>
<body>

<div id="wrap">

<div id="header">
<h1><a href="#">GDR RADIA -- Groupe de Travail Explicabilité et Confiance</a></h1>
<h2>EXPLICON</h2>
</div>


<div id="contentbar"> 

<div class="contentbarleft">

</div>

<div class="contentbarright">

</div>

</div>

<div id="content">

<div class="right"> 

<h2><a href="#">Contexte</a></h2>
Les questions d’explicabilité et de confiance s’inscrivent dans l’objectif plus général d’obtenir une
“IA de confiance” (trustworthy AI). L’avènement des réseaux de neurones, s’il a permis de grande avancées
dans de nombreux domaines de l’IA, a aussi remis sur le devant de la scène le besoin soit de fournir des
garanties sur les prédictions fournies, soit de les assortir d’une quantification d’incertitude qui ait du sens.
Il est en effet courant de voir des réseaux de neurones et d’autres modèles donner une confiance irraisonnée
à des prédictions manifestement fausses, par exemple sous le coup d’attaques adversariables. Cela
a remis au coeur de l’IA d’anciennes problématiques qu’étaient la quantification d’incertitude ou encore la
certification des modèles. Sans que cela soit équivalent, il est aussi parfois nécessaire de pouvoir fournir
des explications. Ces besoins sont particulièrement criants pour des applications sensibles comme le diagnostic
médical, ou économiquement importante comme la détection d’anomalies dans les procédés industriels.

<br>
 Plusieurs pistes de recherche liées à ces besoins trouvent naturellement leurs places au sein du
GDR IA: 
 
 <br>
 <br>
    
    <ul>
       
   <li class="nolist"> le besoin dans certains contextes de fournir des explications permettant à une interaction de nature
collaborative ou ”contestative” de se tenir, ce qui nécessite d’avoir des explications réllement adapatées
à l’utilisateur (qui peut avoir des profils très variés); </li>
     
  <br>
     
   <li class="nolist">  le besoin de considérer des modèles d’incertitudes
riches, incluant à la fois les approches ensemblistes/logiques et les approches statistiques. De nombreux
résultats récents (et moins récents) indiquent que la considération des seules probabilités est insuffisant pour
fournir des quantification d’incertitudes ayant de fortes garanties statistiques. L’utilisation de tels modèles
est une des spécificité des membres du GDR; </li>
     
     <br>
     
     <li class="nolist"> le besoin de certifier que les modèles fournis par l’IA
puissent fonctionner dans des domaines de validités spécifiés, par exemple quand les conditions de fonctionnement
sont susceptibles d’évoluer (transfert des cas simulés ou réalisés en environnement contrôlés
vers un environnement ouvert/réel/non-contrôlés). Cette certification peut naturellement se faire via des
approches logiques ou utilisant des contraintes; </li> 
     
     
     <br>
     
      <li class="nolist">  le besoin de gérer de manière aussi transparente et
robuste que possible l’apparition de conflits et d’incohérence au sein de certains modèles, qu’ils soient de
nature plus symbolique ou plus numérique;</li> 
     
     <br>
     
        <li class="nolist">     
   le besoin de fournir des explications avec garanties et adaptées
à l’interlocuteur. </li>  
       
     </ul>   
   <br>
 <br>

<h2><a href="#">Perspectives et Défis</a></h2>

 <br>
 
    <ul>
     <li> Les méthodes d’explication les plus utilisées (LIME, SHAP) sont de nature heuristique.
Plus récemment, des approches basées sur méthodes formelles, à base d’encodage en PLNE ou SAT,
ont été proposées. Quel est le périmètre d’application de ces méthodes, et quid du passage à l’échelle? </li>
     <br>
     
     <li> La plupart des méthodes d’explication ne permettent pas à l’utilisateur de contester la décision. Comment
prendre en compte l’aspect dialectique des explications? et quel lien peut on mettre avec les mécanismes
d’apprentissage?</li>
     <br>
     
     <li> Comment définir les domaines de validités des modèles (e.g., sous quelles
perturbations un réseau de neurones doit-il continuer à fonctionner?) et comment s’assurer que le modèle
soit valide dans ce domaine?</li>
     
     <br>
     
     <li> Traditionnellement, la performance du système (c’est-à-dire la précision)
est considérée comme la principale mesure lors de l’utilisation d’un modèle d’IA. Mais cette métrique est
loin d’être suffisante pour refléter la fiabilité des systèmes d’IA. Divers aspects devraient être pris en compte
pour améliorer la fiabilité des systèmes d’IA et augmenter la confiance des utilisateurs, y compris, mais sans
s’y limiter, leur robustesse, l’équité algorithmique, l’explicabilité et la transparence. Quels critères, démarche
mettre en place pour mesurer et évaluer ces aspects?</li>
     
    <br>
     <li> Au-delà de la quantification d’incertitude via des
outils mathématiques, comment communiquer cette incertitude à l’utilisateur ou encore éliciter les besoins
de l’utilisateur en terme de ceritfication ou quantification d’incertitudes?</li>
     
     <br>
     
     <li>Comment mélanger explicabilité
et prise en compte de l’incertitude dans les modèles d’IA? Comment s’assurer que les explications soient
elle aussi robuste (qu’une explication valide le reste dans un certain périmètre)</li>
 </ul>
 



</div>

<div class="left"> 

<h2>Menu</h2>

<ul>
    <li><a href="index.html" class="active">ACCUEIL</a></li> 
    <li><a href="resume.html">PERSPECTIVES & DEFIS</a></li> 
    <li><a href="activities.html">EVENEMENTS</a></li> 
    <li><a href="members.html">MEMBRES</a></li> 
     </ul>



<h2>Archives Evénements</h2>
<ul>
<li><a href="http://intranet.gdr-isis.fr/index.php?page=reunion&idreunion=491">January 2023</a></li>
<li><a href="https://imagin.sciencesconf.org/">May 2023</a></li>
<li><a href="https://gt-explicon.github.io/2023_Journees_juin.html">June 2023</a></li>
<li><a href="###">July 2023</a></li>
<li><a href="https://www.cril.univ-artois.fr/ecsqaru23/workshop/">September 2023</a></li>
<li><a href="https://sites.google.com/view/explainai-2024/accueil?authuser=0">January 2024</a></li>
</ul>

</div>

<div style="clear: both;"> </div>

</div>

<div id="footer">
Designed by <a href="http://www.free-css-templates.com/">Free CSS Templates</a>, Thanks to <a href="http://www.openwebdesign.org/">Web Design Dubai</a>
</div>

</div>

</body>
</html>
